{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1759eca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100 / 1000, Loss: 0.5845]\n",
      "Epoch [200 / 1000, Loss: 0.5058]\n",
      "Epoch [300 / 1000, Loss: 0.4429]\n",
      "Epoch [400 / 1000, Loss: 0.3921]\n",
      "Epoch [500 / 1000, Loss: 0.3505]\n",
      "Epoch [600 / 1000, Loss: 0.3161]\n",
      "Epoch [700 / 1000, Loss: 0.2873]\n",
      "Epoch [800 / 1000, Loss: 0.2630]\n",
      "Epoch [900 / 1000, Loss: 0.2422]\n",
      "Epoch [1000 / 1000, Loss: 0.2242]\n",
      "tensor([0., 1., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "X = torch.tensor([[0.5, 0.3], [0.2, 0.8], [0.7, 0.9], [0.1, 0.4]], dtype=torch.float32)\n",
    "y = torch.tensor([0,1,1,0], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "model = Perceptron(input_dim)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    outputs = model(X)\n",
    "\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1} / {epochs}, Loss: {loss.item():.4f}]\")\n",
    "        \n",
    "with torch.no_grad():\n",
    "    predictions = model(X)\n",
    "    predicted_classes = (predictions > 0.5).float()\n",
    "    print(predicted_classes.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a257fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.0059\n",
      "Epoch [200/1000], Loss: 0.0022\n",
      "Epoch [300/1000], Loss: 0.0012\n",
      "Epoch [400/1000], Loss: 0.0008\n",
      "Epoch [500/1000], Loss: 0.0005\n",
      "Epoch [600/1000], Loss: 0.0004\n",
      "Epoch [700/1000], Loss: 0.0003\n",
      "Epoch [800/1000], Loss: 0.0002\n",
      "Epoch [900/1000], Loss: 0.0002\n",
      "Epoch [1000/1000], Loss: 0.0002\n",
      "Predicted classes: tensor([0., 1., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.activation2 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.activation2(x)\n",
    "        return x\n",
    "    \n",
    "X = torch.tensor([[0.5, 0.3], [0.2, 0.8], [0.7, 0.9], [0.1, 0.4]], dtype=torch.float32)\n",
    "y = torch.tensor([0, 1, 1, 0], dtype=torch.float32).view(-1, 1)  # Binary labels\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "hidden_dim = 4\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "model = MultiLayerPerceptron(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "epochs  = 1000\n",
    "for epoch in range(epochs):\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    predictions = model(X)\n",
    "    predicted_classes = (predictions > 0.5).float()\n",
    "    print(\"Predicted classes:\", predicted_classes.view(-1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90580cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000, Loss: 0.0049 ]\n",
      "Epoch [200/1000, Loss: 0.0019 ]\n",
      "Epoch [300/1000, Loss: 0.0011 ]\n",
      "Epoch [400/1000, Loss: 0.0007 ]\n",
      "Epoch [500/1000, Loss: 0.0005 ]\n",
      "Epoch [600/1000, Loss: 0.0004 ]\n",
      "Epoch [700/1000, Loss: 0.0003 ]\n",
      "Epoch [800/1000, Loss: 0.0003 ]\n",
      "Epoch [900/1000, Loss: 0.0002 ]\n",
      "Epoch [1000/1000, Loss: 0.0002 ]\n",
      "tensor([0., 1., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(hidden_dim2, output_dim)\n",
    "        self.activation3 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.activation3(x)\n",
    "        return x \n",
    "    \n",
    "    \n",
    "\n",
    "X = torch.tensor([[0.5, 0.3], [0.2, 0.8], [0.7, 0.9], [0.1, 0.4]], dtype=torch.float32)\n",
    "y = torch.tensor([0, 1, 1, 0], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "hidden_dim1 =4\n",
    "hidden_dim2 = 4\n",
    "output_dim = 1\n",
    "\n",
    "model = FeedForwardNetwork(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "epoch = 1000\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    # calculate gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}, Loss: {loss.item():.4f} ]\")\n",
    "        \n",
    "        \n",
    "with torch.inference_mode():\n",
    "    pred = model(X)\n",
    "    predicted_classes = (pred > 0.5).float()\n",
    "    print(predicted_classes.view(-1))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "63010fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss:  0.1588\n",
      "Epoch 2/10, Loss:  0.0468\n",
      "Epoch 3/10, Loss:  0.0323\n",
      "Epoch 4/10, Loss:  0.0240\n",
      "Epoch 5/10, Loss:  0.0170\n",
      "Epoch 6/10, Loss:  0.0138\n",
      "Epoch 7/10, Loss:  0.0118\n",
      "Epoch 8/10, Loss:  0.0091\n",
      "Epoch 9/10, Loss:  0.0088\n",
      "Epoch 10/10, Loss:  0.0071\n",
      "Test Accuracy:  99.05%\n"
     ]
    }
   ],
   "source": [
    "# Naive CNN\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "class NaiveCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NaiveCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, ), (0.5, ))\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = NaiveCNN().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train() \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss/len(train_loader): .4f}\")\n",
    "    \n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    " \n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "        total += pred.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        \n",
    "print(f\"Test Accuracy: {100 * correct / total: .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f81d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit 0: 0.9460\n",
      "Digit 1: 0.0000\n",
      "Digit 2: 0.0038\n",
      "Digit 3: 0.0042\n",
      "Digit 4: 0.0000\n",
      "Digit 5: 0.0192\n",
      "Digit 6: 0.0005\n",
      "Digit 7: 0.0247\n",
      "Digit 8: 0.0016\n",
      "Digit 9: 0.0000\n",
      "Predicted digit: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4898/404531785.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pth\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NaiveCNN().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "image_path = \"image.png\"\n",
    "image = Image.open(image_path).convert(\"L\")  \n",
    "image = transform(image)\n",
    "\n",
    "\n",
    "image = image.unsqueeze(0).to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output = model(image)\n",
    "    probs = nn.functional.softmax(output, dim=1)\n",
    "    _, predicted = torch.max(probs, 1)\n",
    "    for i, prob in enumerate(probs[0]):\n",
    "        print(f\"Digit {i}: {prob.item():.4f}\")\n",
    "print(f\"Predicted digit: {predicted.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be03f177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 0.1862\n",
      "Epoch [2/20], Loss: 0.5716\n",
      "Epoch [4/20], Loss: 0.1921\n",
      "Epoch [4/20], Loss: 0.3321\n",
      "Epoch [6/20], Loss: 0.1127\n",
      "Epoch [6/20], Loss: 0.1786\n",
      "Epoch [8/20], Loss: 0.0415\n",
      "Epoch [8/20], Loss: 0.1108\n",
      "Epoch [10/20], Loss: 0.0343\n",
      "Epoch [10/20], Loss: 0.0539\n",
      "Epoch [12/20], Loss: 0.0208\n",
      "Epoch [12/20], Loss: 0.0337\n",
      "Epoch [14/20], Loss: 0.0140\n",
      "Epoch [14/20], Loss: 0.0231\n",
      "Epoch [16/20], Loss: 0.0076\n",
      "Epoch [16/20], Loss: 0.0191\n",
      "Epoch [18/20], Loss: 0.0077\n",
      "Epoch [18/20], Loss: 0.0132\n",
      "Epoch [20/20], Loss: 0.0061\n",
      "Epoch [20/20], Loss: 0.0108\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "    \n",
    "# Example data (padded sequences of word indices)\n",
    "sequences = [\n",
    "    [1, 2, 3, 0, 0],  # Sequence 1 (padded with 0s)\n",
    "    [4, 5, 6, 7, 0],  # Sequence 2\n",
    "    [8, 9, 0, 0, 0]   # Sequence 3\n",
    "]\n",
    "labels = [0, 1, 0]  # Binary labels\n",
    "\n",
    "sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "labels = torch.tensor(labels, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "dataset = SimpleDataset(sequences, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size,embed_dim, hidden_dim, output_dim):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embeddings(x)\n",
    "        rnn_out, hidden = self.rnn(embedded)\n",
    "        final_hidden = hidden[-1]\n",
    "        out = self.fc(final_hidden)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "vocab_size = 10\n",
    "embed_dim = 8\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "\n",
    "model = SimpleRNN(vocab_size, embed_dim, hidden_dim, output_dim)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for seq, label in dataloader:\n",
    "        outputs = model(seq)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if ((epoch+1)%2) == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    test_seq = torch.tensor([[4, 5, 6, 7, 0]])\n",
    "    prediction = model(test_seq)\n",
    "    pred_class = (prediction > 0.5).float()\n",
    "    print(pred_class.item())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6bb37dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/10], Loss: 0.6965\n",
      "Epoch [2/10], Loss: 0.6927\n",
      "Epoch [3/10], Loss: 0.6913\n",
      "Epoch [4/10], Loss: 0.6888\n",
      "Epoch [5/10], Loss: 0.6835\n",
      "Epoch [6/10], Loss: 0.6712\n",
      "Epoch [7/10], Loss: 0.6391\n",
      "Epoch [8/10], Loss: 0.5595\n",
      "Epoch [9/10], Loss: 0.4086\n",
      "Epoch [10/10], Loss: 0.2272\n",
      "Test input: 'it's a stunning movie'\n",
      "Predicted class: Negative\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define the dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_seq_size):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_seq_size = max_seq_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.texts[idx].split(\" \")\n",
    "        sequence = [self.vocab[token] if token in self.vocab else self.vocab[\"<UNK>\"] for token in tokens]\n",
    "\n",
    "        # Pad or truncate to match the max sequence size\n",
    "        if len(sequence) < self.max_seq_size:\n",
    "            sequence += [self.vocab[\"<PAD>\"]] * (self.max_seq_size - len(sequence))\n",
    "        elif len(sequence) > self.max_seq_size:\n",
    "            sequence = sequence[:self.max_seq_size]\n",
    "\n",
    "        return torch.tensor(sequence, dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "\n",
    "\n",
    "def build_vocab(texts):\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    index = 2\n",
    "\n",
    "    for text in texts:\n",
    "        for token in text.split(\" \"):\n",
    "            if token not in vocab:\n",
    "                vocab[token] = index\n",
    "                index += 1\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# Step 2: Generate synthetic dataset\n",
    "np.random.seed(0)\n",
    "\n",
    "positive_texts = [\n",
    "    \"I loved the movie it was fantastic\",\n",
    "    \"Absolutely wonderful experience\",\n",
    "    \"Highly recommend this movie\",\n",
    "    \"The movie was amazing\",\n",
    "    \"I enjoyed the film\",\n",
    "    \"The actors were great\",\n",
    "    \"The plot was engaging\",\n",
    "    \"The movie was well-made\",\n",
    "    \"I would watch it again\",\n",
    "    \"The film was outstanding\"\n",
    "]\n",
    "\n",
    "negative_texts = [\n",
    "    \"The film was terrible and boring\",\n",
    "    \"Worst movie I have ever seen\",\n",
    "    \"The movie was disappointing\",\n",
    "    \"I didn't like the film\",\n",
    "    \"The actors were bad\",\n",
    "    \"The plot was confusing\",\n",
    "    \"The movie was poorly made\",\n",
    "    \"I wouldn't watch it again\",\n",
    "    \"The film was average\",\n",
    "    \"The movie was not good\"\n",
    "]\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    if np.random.rand() > 0.5:\n",
    "        texts.append(np.random.choice(positive_texts))\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        texts.append(np.random.choice(negative_texts))\n",
    "        labels.append(0)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"text\": texts,\n",
    "    \"label\": labels\n",
    "})\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split the DataFrame into training and testing sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_texts = df[\"text\"][:train_size].tolist()\n",
    "train_labels = df[\"label\"][:train_size].tolist()\n",
    "test_texts = df[\"text\"][train_size:].tolist()\n",
    "test_labels = df[\"label\"][train_size:].tolist()\n",
    "\n",
    "# Use the training set for your model\n",
    "texts = train_texts\n",
    "labels = train_labels\n",
    "\n",
    "# Step 3: Define the GRU Model\n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embeddings(x)\n",
    "        gru_out, hidden = self.gru(emb)\n",
    "        final_hidden = hidden[-1]\n",
    "        out = self.fc(final_hidden)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Step 4: Build Vocabulary and Hyperparameters\n",
    "vocab = build_vocab(texts)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "max_len = 10\n",
    "embed_dim = 8\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TextDataset(texts=texts, labels=labels, vocab=vocab, max_seq_size=max_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model and move it to the device\n",
    "model = GRUNet(vocab_size, embed_dim, hidden_dim, output_dim).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for seq, label in dataloader:\n",
    "        # Move inputs and labels to the device\n",
    "        seq, label = seq.to(device), label.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(seq)\n",
    "        loss = criterion(y_pred, label.unsqueeze(1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"gru_model.pth\")\n",
    "\n",
    "# Testing the model\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    test_sentence = \"it's a stunning movie\"\n",
    "    tokens = test_sentence.split(\" \")\n",
    "    seq = [vocab[token] if token in vocab else vocab[\"<UNK>\"] for token in tokens]\n",
    "    seq = seq[:max_len] + [vocab[\"<PAD>\"]] * (max_len - len(seq))\n",
    "    test_seq = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)  # Move to device\n",
    "    y_pred = model(test_seq)\n",
    "    pred_class = (y_pred > 0.5).float()\n",
    "    print(f\"Test input: '{test_sentence}'\")\n",
    "    print(f\"Predicted class: {'Positive' if pred_class.item() == 1 else 'Negative'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c919427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from collections import Counter \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "027ed259",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a655e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b2f38a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[one, of, the, other, reviewers, has, mentione...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[a, wonderful, little, production., &lt;br, /&gt;&lt;br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[i, thought, this, was, a, wonderful, way, to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[basically, there's, a, family, where, a, litt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[petter, mattei's, \"love, in, the, time, of, m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                              tokens  \n",
       "0  [one, of, the, other, reviewers, has, mentione...  \n",
       "1  [a, wonderful, little, production., <br, /><br...  \n",
       "2  [i, thought, this, was, a, wonderful, way, to,...  \n",
       "3  [basically, there's, a, family, where, a, litt...  \n",
       "4  [petter, mattei's, \"love, in, the, time, of, m...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"r'[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "df[\"tokens\"] = df[\"review\"].apply(preprocess_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36440cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[one, of, the, other, reviewers, has, mentione...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[a, wonderful, little, production., &lt;br, /&gt;&lt;br...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[i, thought, this, was, a, wonderful, way, to,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[basically, there's, a, family, where, a, litt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[petter, mattei's, \"love, in, the, time, of, m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                              tokens  label  \n",
       "0  [one, of, the, other, reviewers, has, mentione...      1  \n",
       "1  [a, wonderful, little, production., <br, /><br...      1  \n",
       "2  [i, thought, this, was, a, wonderful, way, to,...      1  \n",
       "3  [basically, there's, a, family, where, a, litt...      0  \n",
       "4  [petter, mattei's, \"love, in, the, time, of, m...      1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocab(tokenized_texts, max_vocab_size=10000):\n",
    "    words_count = Counter(word for tokens in tokenized_texts for word in tokens)\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for word, _ in words_count.most_common(max_vocab_size - 2):\n",
    "        vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "vocab = build_vocab(df[\"tokens\"])\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "label_map = {\"positive\": 1, \"negative\": 0}\n",
    "df[\"label\"] = df[\"sentiment\"].map(label_map)\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01699cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df[\"tokens\"], df[\"label\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c9dbe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.texts.iloc[idx]\n",
    "        sequence = [self.vocab[token] if token in self.vocab else self.vocab[\"<UNK>\"] for token in tokens]\n",
    "\n",
    "        # Pad or truncate to match the max sequence size\n",
    "        if len(sequence) < self.max_len:\n",
    "            sequence += [self.vocab[\"<PAD>\"]] * (self.max_len - len(sequence))\n",
    "        elif len(sequence) > self.max_len:\n",
    "            sequence = sequence[:self.max_len]\n",
    "\n",
    "        return torch.tensor(sequence, dtype=torch.long), torch.tensor(self.labels.iloc[idx], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b834bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 200\n",
    "embed_dim = 128\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9db3a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(train_texts, train_labels, vocab=vocab, max_len=max_seq_len)\n",
    "test_dataset = IMDBDataset(test_texts, test_labels, vocab=vocab, max_len=max_seq_len)\n",
    "\n",
    "\n",
    "train_data = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_data  = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "028723be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, vocab, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(GRU, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        _, hidden = self.gru(x)\n",
    "        final_hidden = hidden[-1] \n",
    "        output = self.fc(final_hidden)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "082c239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device: \", device)\n",
    "model =  GRU(vocab, vocab_size, embed_dim, hidden_dim, output_dim).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eaf39402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [0/5], Loss: 0.6801\n",
      "Epoch : [1/5], Loss: 0.3894\n",
      "Epoch : [2/5], Loss: 0.2573\n",
      "Epoch : [3/5], Loss: 0.1769\n",
      "Epoch : [4/5], Loss: 0.1000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0 \n",
    "    \n",
    "    for text, label in train_data:\n",
    "        text, label = text.to(device), label.to(device)\n",
    "        \n",
    "        y_pred = model(text)\n",
    "        \n",
    "        loss = criterion(y_pred.squeeze(1), label)\n",
    "        \n",
    "        # reset grads\n",
    "        optimizer.zero_grad()\n",
    "        # calculate grads\n",
    "        loss.backward()\n",
    "        # update grads\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch : [{epoch+1}/{epochs}], Loss: {running_loss/len(train_data):.4f}\")\n",
    "    torch.save(model.state_dict(), f\"gru_model_{epoch+1}.pth\")\n",
    "    \n",
    "torch.save(model.state_dict(), \"imdb_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "85a9d68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76064/2864941614.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(\"imdb_model.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Test Accuracy: 0.8605\n"
     ]
    }
   ],
   "source": [
    "model_state_dict = torch.load(\"imdb_model.pth\")\n",
    "\n",
    "model = GRU(vocab=vocab, vocab_size=vocab_size, embed_dim=embed_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "print(device) \n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text, label in test_data:\n",
    "        text, label  = text.to(device), label.to(device)\n",
    "        y_pred = model(text)\n",
    "        \n",
    "        predictions = (y_pred.squeeze(1) > 0.5).float()\n",
    "        total += label.size(0)\n",
    "        \n",
    "        correct += (predictions == label).sum().item()\n",
    "        \n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f17d5fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, text, vocab, max_len, device):\n",
    "    model.eval()\n",
    "    sequence = [vocab.get(token, vocab[\"<UNK>\"]) for token in text.split()]\n",
    "    if len(sequence) < max_len:\n",
    "        sequence += [vocab[\"<PAD>\"]] * (max_len - len(sequence))\n",
    "    else:\n",
    "        sequence = sequence[:max_len]\n",
    "    \n",
    "    input_tensor = torch.tensor(sequence, dtype=torch.long).unsqueeze(0).to(device)  \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_tensor)\n",
    "    \n",
    "    return prediction.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bcaf2dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment score:  0.0018544727936387062\n",
      "Verdict: 'Negative'\n"
     ]
    }
   ],
   "source": [
    "text = \"This movie is bad but I liked the songs anyway but the movie is worst\"\n",
    "sentiment_score = predict_sentiment(model, text, vocab, max_seq_len, device)\n",
    "print(\"sentiment score: \", sentiment_score)\n",
    "label = \"Positive\" if sentiment_score >= 0.5 else \"Negative\"\n",
    "print(f\"Verdict: '{label}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a1946",
   "metadata": {},
   "source": [
    "#### Seq-2-Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53492a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  English      French\n",
      "0     Go.        Va !\n",
      "1     Go.     Marche.\n",
      "2     Go.  En route !\n",
      "3     Go.     Bouge !\n",
      "4     Hi.     Salut !\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = \"dataset_nmt.csv\"  \n",
    "df = pd.read_csv(file_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "203ca3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "def build_vocab(sentences, max_vocab_size=5000):\n",
    "    word_counts = Counter(word for sentence in sentences for word in sentence)\n",
    "    vocab = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3} \n",
    "    for word, _ in word_counts.most_common(max_vocab_size - 4):\n",
    "        vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def encode_sentences(sentences, vocab, max_len):\n",
    "    encoded = []\n",
    "    for sentence in sentences:\n",
    "        tokens = [vocab.get(token, vocab[\"<UNK>\"]) for token in sentence]\n",
    "        tokens = [vocab[\"<SOS>\"]] + tokens[:max_len - 2] + [vocab[\"<EOS>\"]]\n",
    "        if len(tokens) < max_len:\n",
    "            tokens += [vocab[\"<PAD>\"]] * (max_len - len(tokens))\n",
    "        encoded.append(tokens)\n",
    "    return encoded\n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, input_sequences, output_sequences):\n",
    "        self.input_sequences = input_sequences\n",
    "        self.output_sequences = output_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.input_sequences[idx], dtype=torch.long), torch.tensor(self.output_sequences[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "english_sentences = df[\"English\"].tolist()\n",
    "french_sentences = df[\"French\"].tolist()\n",
    "\n",
    "\n",
    "tokenized_english = [tokenize(sentence) for sentence in english_sentences]\n",
    "tokenized_french = [tokenize(sentence) for sentence in french_sentences]\n",
    "\n",
    "\n",
    "en_vocab = build_vocab(tokenized_english)\n",
    "fr_vocab = build_vocab(tokenized_french)\n",
    "\n",
    "\n",
    "max_len = 20  \n",
    "encoded_english = encode_sentences(tokenized_english, en_vocab, max_len)\n",
    "encoded_french = encode_sentences(tokenized_french, fr_vocab, max_len)\n",
    "\n",
    "\n",
    "train_en, test_en, train_fr, test_fr = train_test_split(\n",
    "    encoded_english, encoded_french, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = TranslationDataset(train_en, train_fr)\n",
    "test_dataset = TranslationDataset(test_en, test_fr)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "162dcf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  \n",
    "        outputs, hidden = self.rnn(embedded) \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "533252ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)  \n",
    "        output, hidden = self.rnn(embedded, hidden)  \n",
    "        prediction = self.fc(output.squeeze(1))  \n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9c378bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.fc.out_features\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        hidden = self.encoder(src)\n",
    "        input = trg[:, 0].unsqueeze(1)  \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1) \n",
    "            input = trg[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b001e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model parameters on: cuda:0\n",
      "Parameter encoder.embedding.weight is on cuda:0, should be on cuda\n",
      "Parameter encoder.rnn.weight_ih_l0 is on cuda:0, should be on cuda\n",
      "Parameter encoder.rnn.weight_hh_l0 is on cuda:0, should be on cuda\n",
      "Parameter encoder.rnn.bias_ih_l0 is on cuda:0, should be on cuda\n",
      "Parameter encoder.rnn.bias_hh_l0 is on cuda:0, should be on cuda\n",
      "Parameter decoder.embedding.weight is on cuda:0, should be on cuda\n",
      "Parameter decoder.rnn.weight_ih_l0 is on cuda:0, should be on cuda\n",
      "Parameter decoder.rnn.weight_hh_l0 is on cuda:0, should be on cuda\n",
      "Parameter decoder.rnn.bias_ih_l0 is on cuda:0, should be on cuda\n",
      "Parameter decoder.rnn.bias_hh_l0 is on cuda:0, should be on cuda\n",
      "Parameter decoder.fc.weight is on cuda:0, should be on cuda\n",
      "Parameter decoder.fc.bias is on cuda:0, should be on cuda\n",
      "Epoch [1/10], Loss: 4.1846\n",
      "Epoch [2/10], Loss: 3.2374\n",
      "Epoch [3/10], Loss: 2.9057\n",
      "Epoch [4/10], Loss: 2.7176\n",
      "Epoch [5/10], Loss: 2.5925\n",
      "Epoch [6/10], Loss: 2.4991\n",
      "Epoch [7/10], Loss: 2.4239\n",
      "Epoch [8/10], Loss: 2.3713\n",
      "Epoch [9/10], Loss: 2.3136\n",
      "Epoch [10/10], Loss: 2.2737\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming en_vocab, fr_vocab, train_loader, Encoder, Decoder, Seq2Seq are already defined\n",
    "\n",
    "input_dim = len(en_vocab)\n",
    "output_dim = len(fr_vocab)\n",
    "embed_dim = 128\n",
    "hidden_dim = 256\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model components\n",
    "encoder = Encoder(input_dim, embed_dim, hidden_dim).to(device)\n",
    "decoder = Decoder(output_dim, embed_dim, hidden_dim).to(device)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "print(f\"Model parameters on: {next(model.parameters()).device}\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=en_vocab[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Confirm all parameters are on GPU\n",
    "for name, param in model.named_parameters():\n",
    "    if param.device != device:\n",
    "        print(f\"Parameter {name} is on {param.device}, should be on {device}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for src, trg in train_loader:\n",
    "        src = src.to(device, non_blocking=True)\n",
    "        trg = trg.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)  # [batch_size, trg_len, output_dim]\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output.view(-1, output_dim)  # [batch_size * trg_len, output_dim]\n",
    "        trg = trg.view(-1)                    # [batch_size * trg_len]\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"seq2seq_translation_model.pth\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a9d699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2788/795637911.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"seq2seq_translation_model.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Recreate the model architecture\n",
    "input_dim = len(en_vocab)\n",
    "output_dim = len(fr_vocab)\n",
    "embed_dim = 128\n",
    "hidden_dim = 256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(input_dim, embed_dim, hidden_dim).to(device)\n",
    "decoder = Decoder(output_dim, embed_dim, hidden_dim).to(device)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(\"seq2seq_translation_model.pth\", map_location=device))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bdb460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, en_vocab, fr_vocab, max_len=20):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize the input sentence\n",
    "    tokens = sentence.lower().split()\n",
    "\n",
    "    # Convert tokens to indices\n",
    "    src_tokens = [en_vocab.get(token, en_vocab[\"<UNK>\"]) for token in tokens]\n",
    "    src_tokens = [en_vocab[\"<SOS>\"]] + src_tokens[:max_len - 2] + [en_vocab[\"<EOS>\"]]\n",
    "\n",
    "    # Pad the sequence if necessary\n",
    "    if len(src_tokens) < max_len:\n",
    "        src_tokens += [en_vocab[\"<PAD>\"]] * (max_len - len(src_tokens))\n",
    "\n",
    "    # Convert to tensor and move to the device\n",
    "    src_tensor = torch.tensor(src_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # Encode the source sequence\n",
    "    with torch.no_grad():\n",
    "        hidden = model.encoder(src_tensor)\n",
    "\n",
    "    # Decode the target sequence\n",
    "    trg_tokens = [fr_vocab[\"<SOS>\"]]\n",
    "    for _ in range(max_len):\n",
    "        trg_tensor = torch.tensor([trg_tokens[-1]], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model.decoder(trg_tensor, hidden)\n",
    "\n",
    "        # Get the predicted token\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_tokens.append(pred_token)\n",
    "\n",
    "        # Stop if we predict the end-of-sequence token\n",
    "        if pred_token == fr_vocab[\"<EOS>\"]:\n",
    "            break\n",
    "\n",
    "    # Convert indices back to words\n",
    "    translated_sentence = \" \".join(\n",
    "        [list(fr_vocab.keys())[list(fr_vocab.values()).index(token)] for token in trg_tokens[1:-1]]\n",
    "    )\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6ba765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I love trying out new things\n",
      "Translated: j'adore les nouvelles de choses.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"I love trying out new things\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    translated_sentence = translate_sentence(sentence, model, en_vocab, fr_vocab)\n",
    "    print(f\"Input: {sentence}\")\n",
    "    print(f\"Translated: {translated_sentence}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "357eeadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Transformer architecture\n",
    "# https://github.com/heissanjay/nlp/tree/main/transformer/notes\n",
    "\n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import DataLoader \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9382ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c611fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # Convert to expected format for MultiheadAttention (seq_len, batch, features)\n",
    "        src = src.transpose(0, 1)\n",
    "        attn_out, _ = self.self_attn(src, src, src)\n",
    "        src = src + self.dropout(attn_out)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        ff_out = F.relu(self.linear1(src))\n",
    "        ff_out = self.dropout(ff_out)\n",
    "        ff_out = self.linear2(ff_out)\n",
    "        src = src + self.dropout(ff_out)\n",
    "        src = self.norm2(src)\n",
    "        \n",
    "        # Convert back to (batch, seq_len, features)\n",
    "        return src.transpose(0, 1)\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.01):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model  # Store d_model as instance variable\n",
    "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encodings = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout=dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        src = self.embeddings(src) * math.sqrt(self.d_model)  # Use self.d_model\n",
    "        src = self.pos_encodings(src)\n",
    "        src = self.dropout(src)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a484a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.enc_dec_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, target, memory, target_mask=None):\n",
    "        # Convert to expected format for MultiheadAttention (seq_len, batch, features)\n",
    "        target = target.transpose(0, 1)\n",
    "        memory = memory.transpose(0, 1)\n",
    "\n",
    "        # Self-attention with mask\n",
    "        attn_out, _ = self.self_attn(target, target, target, attn_mask=target_mask)\n",
    "        target = target + self.dropout(attn_out)\n",
    "        target = self.norm1(target)\n",
    "\n",
    "        # Encoder-decoder attention\n",
    "        attn_out, _ = self.enc_dec_attn(target, memory, memory)\n",
    "        target = target + self.dropout(attn_out)\n",
    "        target = self.norm2(target)\n",
    "\n",
    "        # Feed-forward network\n",
    "        ff_output = F.relu(self.linear1(target))\n",
    "        ff_output = self.dropout(ff_output)\n",
    "        ff_output = self.linear2(ff_output)\n",
    "        target = target + self.dropout(ff_output)\n",
    "        target = self.norm3(target)\n",
    "\n",
    "        # Convert back to (batch, seq_len, features)\n",
    "        return target.transpose(0, 1)  \n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model  # Store d_model as instance variable\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout=dropout) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, target, memory, target_mask=None):\n",
    "        target = self.embedding(target) * math.sqrt(self.d_model)  # Use self.d_model\n",
    "        target = self.pos_encoding(target)\n",
    "        target = self.dropout(target)\n",
    "        for layer in self.layers:\n",
    "            target = layer(target, memory, target_mask)\n",
    "        output = self.fc_out(target)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(TransformerSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder\n",
    "        self.device = device \n",
    "        \n",
    "    def make_target_mask(self, target):\n",
    "        # Create causal mask for decoder self-attention\n",
    "        target_len = target.size(1)\n",
    "        # Create lower triangular mask\n",
    "        mask = torch.triu(torch.ones(target_len, target_len) * float('-inf'), diagonal=1).to(self.device)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src, target):\n",
    "        target_mask = self.make_target_mask(target)\n",
    "        memory = self.encoder(src)\n",
    "        output = self.decoder(target, memory, target_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9fba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "def train_model(model, train_loader, en_vocab, optimizer, criterion, epochs, save_path, clip_value=1.0):\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for src, target in train_loader:\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward pass\n",
    "            output = model(src, target[:, :-1])\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            # compute loss\n",
    "            output = output.reshape(-1, output_dim)\n",
    "            target_labels = target[:, 1:].contiguous().reshape(-1)\n",
    "            loss = criterion(output, target_labels)\n",
    "            \n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epochs [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "        \n",
    "        model_info = {\n",
    "            'state_dict': model.state_dict(),\n",
    "            'config': {\n",
    "                'src_vocab_size': len(en_vocab),\n",
    "                'target_vocab_size': model.decoder.fc_out.out_features,\n",
    "                'd_model': model.encoder.d_model,\n",
    "                'num_heads': model.encoder.layers[0].self_attn.num_heads,\n",
    "                'd_ff': model.encoder.layers[0].linear1.out_features,\n",
    "                'num_layers': len(model.encoder.layers),\n",
    "                'dropout': model.encoder.dropout.p\n",
    "            },\n",
    "            'epoch': epoch+1,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': running_loss/len(train_loader)\n",
    "        }\n",
    "        \n",
    "        # Save complete model every 5 epochs to save space\n",
    "        if (epoch + 1) % 5 == 0 or epoch == epochs - 1:\n",
    "            torch.save(model, os.path.join(save_path, f\"transformer_complete_epoch_{epoch+1}.pt\"))\n",
    "        \n",
    "        # Always save model info\n",
    "        torch.save(model_info, os.path.join(save_path, f\"transformer_info_epoch_{epoch+1}.pt\"))\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model, os.path.join(save_path, \"transformer_complete_final.pt\"))\n",
    "    torch.save(model_info, os.path.join(save_path, \"transformer_info_final.pt\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "src_vocab_size = len(en_vocab)\n",
    "target_vocab_size = len(fr_vocab)\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6\n",
    "dropout = 0.1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(src_vocab_size, d_model, num_heads, d_ff, num_layers, dropout)\n",
    "decoder = Decoder(target_vocab_size, d_model, num_heads, d_ff, num_layers, dropout)\n",
    "model = TransformerSeq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Initialize optimizer with lower learning rate and weight decay\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=en_vocab[\"<PAD>\"])\n",
    "\n",
    "# Create save directory\n",
    "save_path = './model_checkpoints_fixed/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Train model\n",
    "train_model(model, train_loader, en_vocab, optimizer, criterion, epochs=20, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b0dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Define the greedy decoding function\n",
    "def translate_sentence(sentence, model, en_vocab, fr_vocab, max_len=50):\n",
    "    \"\"\"\n",
    "    Translate a single sentence using the trained Transformer model.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): Input sentence in English.\n",
    "        model (TransformerSeq2Seq): Trained Transformer model.\n",
    "        en_vocab (dict): English vocabulary (word-to-index mapping).\n",
    "        fr_vocab (dict): French vocabulary (word-to-index mapping).\n",
    "        max_len (int): Maximum length of the generated sequence.\n",
    "    \n",
    "    Returns:\n",
    "        str: Translated sentence in French.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Tokenize and convert the input sentence to indices\n",
    "    tokens = sentence.lower().split()\n",
    "    src_tokens = [en_vocab.get(token, en_vocab[\"<UNK>\"]) for token in tokens]\n",
    "    src_tensor = torch.tensor(src_tokens, dtype=torch.long).unsqueeze(0).to(device)  # Shape: [1, src_len]\n",
    "    \n",
    "    # Encode the source sequence\n",
    "    memory = model.encoder(src_tensor)\n",
    "    \n",
    "    # Initialize the target sequence with <SOS>\n",
    "    trg_tokens = [fr_vocab[\"<SOS>\"]]\n",
    "    \n",
    "    # Decode step-by-step\n",
    "    for _ in range(max_len):\n",
    "        trg_tensor = torch.tensor(trg_tokens, dtype=torch.long).unsqueeze(0).to(device)  # Shape: [1, trg_len]\n",
    "        \n",
    "        # Create the target mask for the current sequence\n",
    "        target_mask = model.make_target_mask(trg_tensor)\n",
    "        \n",
    "        # Pass through the decoder\n",
    "        with torch.no_grad():\n",
    "            output = model.decoder(trg_tensor, memory, target_mask)\n",
    "        \n",
    "        # Get the predicted token (argmax over the vocabulary)\n",
    "        pred_token = output.argmax(dim=-1)[:, -1].item()  # Take the last token's prediction\n",
    "        \n",
    "        # Append the predicted token to the target sequence\n",
    "        trg_tokens.append(pred_token)\n",
    "        \n",
    "        # Stop if <EOS> token is generated\n",
    "        if pred_token == fr_vocab[\"<EOS>\"]:\n",
    "            break\n",
    "    \n",
    "    # Convert indices back to words\n",
    "    translated_sentence = \" \".join(\n",
    "        [list(fr_vocab.keys())[list(fr_vocab.values()).index(token)] for token in trg_tokens[1:-1]]  # Exclude <SOS> and <EOS>\n",
    "    )\n",
    "    return translated_sentence\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths\n",
    "    save_path = './model_checkpoints/'\n",
    "    final_model_path = os.path.join(save_path, \"transformer_info_epoch_6.pt\")\n",
    "    \n",
    "    # Check if the saved model exists\n",
    "    if not os.path.exists(final_model_path):\n",
    "        raise FileNotFoundError(f\"Model checkpoint not found at {final_model_path}\")\n",
    "    \n",
    "    # Load the model and configuration\n",
    "    checkpoint = torch.load(final_model_path, map_location=torch.device('cpu'))  # Use 'cpu' if no GPU\n",
    "    config = checkpoint['config']\n",
    "    \n",
    "    # Rebuild the model architecture\n",
    "    encoder = Encoder(\n",
    "        config['src_vocab_size'], config['d_model'], config['num_heads'], config['d_ff'], config['num_layers'], config['dropout']\n",
    "    )\n",
    "    decoder = Decoder(\n",
    "        config['target_vocab_size'], config['d_model'], config['num_heads'], config['d_ff'], config['num_layers'], config['dropout']\n",
    "    )\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TransformerSeq2Seq(encoder, decoder, device).to(device)\n",
    "    \n",
    "    # Load the state dictionary\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    # Test sentences\n",
    "    test_sentences = [\n",
    "        \"hello how are you\",\n",
    "        \"what is your name\",\n",
    "        \"i love programming\",\n",
    "        \"can you help me\"\n",
    "    ]\n",
    "    \n",
    "    # Translate each sentence\n",
    "    for sentence in test_sentences:\n",
    "        translated_sentence = translate_sentence(sentence, model, en_vocab, fr_vocab)\n",
    "        print(f\"Input: {sentence}\")\n",
    "        print(f\"Translated: {translated_sentence}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa9a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "def beam_search(sentence, model, en_vocab, fr_vocab, max_len=50, beam_size=5):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and convert the input sentence to indices\n",
    "    tokens = sentence.lower().split()\n",
    "    src_tokens = [en_vocab.get(token, en_vocab[\"<UNK>\"]) for token in tokens]\n",
    "    src_tensor = torch.tensor(src_tokens, dtype=torch.long).unsqueeze(0).to(next(model.parameters()).device)\n",
    "\n",
    "    # Encode the source sequence\n",
    "    memory = model.encoder(src_tensor)\n",
    "\n",
    "    # Initialize beam\n",
    "    beams = [([fr_vocab[\"<SOS>\"]], 0.0)]  # [(sequence, log_prob)]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        new_beams = []\n",
    "\n",
    "        for seq, score in beams:\n",
    "            if seq[-1] == fr_vocab[\"<EOS>\"]:\n",
    "                new_beams.append((seq, score))  # Keep finished sequences\n",
    "                continue\n",
    "\n",
    "            trg_tensor = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(next(model.parameters()).device)\n",
    "            target_mask = model.make_target_mask(trg_tensor)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model.decoder(trg_tensor, memory, target_mask)\n",
    "\n",
    "            log_probs = torch.log_softmax(output[:, -1, :], dim=-1)\n",
    "            topk_probs, topk_indices = log_probs.topk(beam_size, dim=-1)\n",
    "\n",
    "            for prob, idx in zip(topk_probs[0], topk_indices[0]):\n",
    "                new_seq = seq + [idx.item()]\n",
    "                new_score = score + prob.item()\n",
    "                new_beams.append((new_seq, new_score))\n",
    "\n",
    "        # Select top-k beams\n",
    "        new_beams.sort(key=lambda x: x[1], reverse=True)\n",
    "        beams = new_beams[:beam_size]\n",
    "\n",
    "        # Stop if all beams are finished\n",
    "        if all(seq[-1] == fr_vocab[\"<EOS>\"] for seq, _ in beams):\n",
    "            break\n",
    "\n",
    "    # Select the best sequence\n",
    "    best_seq, _ = max(beams, key=lambda x: x[1])\n",
    "\n",
    "    # Convert indices back to words\n",
    "    translated_sentence = \" \".join(\n",
    "        [list(fr_vocab.keys())[list(fr_vocab.values()).index(token)] for token in best_seq[1:-1] if token != fr_vocab[\"<EOS>\"]]\n",
    "    )\n",
    "    return translated_sentence\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths\n",
    "    save_path = './model_checkpoints_fixed_1/'\n",
    "    final_model_path = os.path.join(save_path, \"transformer_info_final.pt\")\n",
    "\n",
    "    # Check if the saved model exists\n",
    "    if not os.path.exists(final_model_path):\n",
    "        raise FileNotFoundError(f\"Model checkpoint not found at {final_model_path}\")\n",
    "\n",
    "    # Load the model and configuration\n",
    "    checkpoint = torch.load(final_model_path, map_location=torch.device('cpu'))  # Use 'cpu' if no GPU\n",
    "    config = checkpoint['config']\n",
    "\n",
    "    # Rebuild the model architecture\n",
    "    encoder = Encoder(\n",
    "        config['src_vocab_size'], config['d_model'], config['num_heads'], config['d_ff'], config['num_layers'], config['dropout']\n",
    "    )\n",
    "    decoder = Decoder(\n",
    "        config['target_vocab_size'], config['d_model'], config['num_heads'], config['d_ff'], config['num_layers'], config['dropout']\n",
    "    )\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TransformerSeq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "    # Load the state dictionary\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.to(device)\n",
    "\n",
    "    # Test sentences\n",
    "    test_sentences = [\n",
    "        \"I love data science\",\n",
    "        \"Hello There\",\n",
    "        \"I love new things\",\n",
    "        \"Help me out\"\n",
    "    ]\n",
    "\n",
    "    # Translate each sentence\n",
    "    for sentence in test_sentences:\n",
    "        translated_sentence = beam_search(sentence, model, en_vocab, fr_vocab)\n",
    "        print(f\"Input: {sentence}\")\n",
    "        print(f\"Translated: {translated_sentence}\")\n",
    "        print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
