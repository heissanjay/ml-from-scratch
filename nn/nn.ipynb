{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1759eca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100 / 1000, Loss: 0.5845]\n",
      "Epoch [200 / 1000, Loss: 0.5058]\n",
      "Epoch [300 / 1000, Loss: 0.4429]\n",
      "Epoch [400 / 1000, Loss: 0.3921]\n",
      "Epoch [500 / 1000, Loss: 0.3505]\n",
      "Epoch [600 / 1000, Loss: 0.3161]\n",
      "Epoch [700 / 1000, Loss: 0.2873]\n",
      "Epoch [800 / 1000, Loss: 0.2630]\n",
      "Epoch [900 / 1000, Loss: 0.2422]\n",
      "Epoch [1000 / 1000, Loss: 0.2242]\n",
      "tensor([0., 1., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "X = torch.tensor([[0.5, 0.3], [0.2, 0.8], [0.7, 0.9], [0.1, 0.4]], dtype=torch.float32)\n",
    "y = torch.tensor([0,1,1,0], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "model = Perceptron(input_dim)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    outputs = model(X)\n",
    "\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1} / {epochs}, Loss: {loss.item():.4f}]\")\n",
    "        \n",
    "with torch.no_grad():\n",
    "    predictions = model(X)\n",
    "    predicted_classes = (predictions > 0.5).float()\n",
    "    print(predicted_classes.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a257fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.0059\n",
      "Epoch [200/1000], Loss: 0.0022\n",
      "Epoch [300/1000], Loss: 0.0012\n",
      "Epoch [400/1000], Loss: 0.0008\n",
      "Epoch [500/1000], Loss: 0.0005\n",
      "Epoch [600/1000], Loss: 0.0004\n",
      "Epoch [700/1000], Loss: 0.0003\n",
      "Epoch [800/1000], Loss: 0.0002\n",
      "Epoch [900/1000], Loss: 0.0002\n",
      "Epoch [1000/1000], Loss: 0.0002\n",
      "Predicted classes: tensor([0., 1., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.activation2 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.activation2(x)\n",
    "        return x\n",
    "    \n",
    "X = torch.tensor([[0.5, 0.3], [0.2, 0.8], [0.7, 0.9], [0.1, 0.4]], dtype=torch.float32)\n",
    "y = torch.tensor([0, 1, 1, 0], dtype=torch.float32).view(-1, 1)  # Binary labels\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "hidden_dim = 4\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "model = MultiLayerPerceptron(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "epochs  = 1000\n",
    "for epoch in range(epochs):\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    predictions = model(X)\n",
    "    predicted_classes = (predictions > 0.5).float()\n",
    "    print(\"Predicted classes:\", predicted_classes.view(-1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90580cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000, Loss: 0.0049 ]\n",
      "Epoch [200/1000, Loss: 0.0019 ]\n",
      "Epoch [300/1000, Loss: 0.0011 ]\n",
      "Epoch [400/1000, Loss: 0.0007 ]\n",
      "Epoch [500/1000, Loss: 0.0005 ]\n",
      "Epoch [600/1000, Loss: 0.0004 ]\n",
      "Epoch [700/1000, Loss: 0.0003 ]\n",
      "Epoch [800/1000, Loss: 0.0003 ]\n",
      "Epoch [900/1000, Loss: 0.0002 ]\n",
      "Epoch [1000/1000, Loss: 0.0002 ]\n",
      "tensor([0., 1., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(hidden_dim2, output_dim)\n",
    "        self.activation3 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.activation3(x)\n",
    "        return x \n",
    "    \n",
    "    \n",
    "\n",
    "X = torch.tensor([[0.5, 0.3], [0.2, 0.8], [0.7, 0.9], [0.1, 0.4]], dtype=torch.float32)\n",
    "y = torch.tensor([0, 1, 1, 0], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "hidden_dim1 =4\n",
    "hidden_dim2 = 4\n",
    "output_dim = 1\n",
    "\n",
    "model = FeedForwardNetwork(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "epoch = 1000\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    # calculate gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}, Loss: {loss.item():.4f} ]\")\n",
    "        \n",
    "        \n",
    "with torch.inference_mode():\n",
    "    pred = model(X)\n",
    "    predicted_classes = (pred > 0.5).float()\n",
    "    print(predicted_classes.view(-1))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "63010fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss:  0.1588\n",
      "Epoch 2/10, Loss:  0.0468\n",
      "Epoch 3/10, Loss:  0.0323\n",
      "Epoch 4/10, Loss:  0.0240\n",
      "Epoch 5/10, Loss:  0.0170\n",
      "Epoch 6/10, Loss:  0.0138\n",
      "Epoch 7/10, Loss:  0.0118\n",
      "Epoch 8/10, Loss:  0.0091\n",
      "Epoch 9/10, Loss:  0.0088\n",
      "Epoch 10/10, Loss:  0.0071\n",
      "Test Accuracy:  99.05%\n"
     ]
    }
   ],
   "source": [
    "# Naive CNN\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "class NaiveCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NaiveCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, ), (0.5, ))\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = NaiveCNN().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train() \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss/len(train_loader): .4f}\")\n",
    "    \n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    " \n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "        total += pred.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        \n",
    "print(f\"Test Accuracy: {100 * correct / total: .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f81d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit 0: 0.9460\n",
      "Digit 1: 0.0000\n",
      "Digit 2: 0.0038\n",
      "Digit 3: 0.0042\n",
      "Digit 4: 0.0000\n",
      "Digit 5: 0.0192\n",
      "Digit 6: 0.0005\n",
      "Digit 7: 0.0247\n",
      "Digit 8: 0.0016\n",
      "Digit 9: 0.0000\n",
      "Predicted digit: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4898/404531785.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pth\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NaiveCNN().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "image_path = \"image.png\"\n",
    "image = Image.open(image_path).convert(\"L\")  \n",
    "image = transform(image)\n",
    "\n",
    "\n",
    "image = image.unsqueeze(0).to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output = model(image)\n",
    "    probs = nn.functional.softmax(output, dim=1)\n",
    "    _, predicted = torch.max(probs, 1)\n",
    "    for i, prob in enumerate(probs[0]):\n",
    "        print(f\"Digit {i}: {prob.item():.4f}\")\n",
    "print(f\"Predicted digit: {predicted.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be03f177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 0.1862\n",
      "Epoch [2/20], Loss: 0.5716\n",
      "Epoch [4/20], Loss: 0.1921\n",
      "Epoch [4/20], Loss: 0.3321\n",
      "Epoch [6/20], Loss: 0.1127\n",
      "Epoch [6/20], Loss: 0.1786\n",
      "Epoch [8/20], Loss: 0.0415\n",
      "Epoch [8/20], Loss: 0.1108\n",
      "Epoch [10/20], Loss: 0.0343\n",
      "Epoch [10/20], Loss: 0.0539\n",
      "Epoch [12/20], Loss: 0.0208\n",
      "Epoch [12/20], Loss: 0.0337\n",
      "Epoch [14/20], Loss: 0.0140\n",
      "Epoch [14/20], Loss: 0.0231\n",
      "Epoch [16/20], Loss: 0.0076\n",
      "Epoch [16/20], Loss: 0.0191\n",
      "Epoch [18/20], Loss: 0.0077\n",
      "Epoch [18/20], Loss: 0.0132\n",
      "Epoch [20/20], Loss: 0.0061\n",
      "Epoch [20/20], Loss: 0.0108\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "    \n",
    "# Example data (padded sequences of word indices)\n",
    "sequences = [\n",
    "    [1, 2, 3, 0, 0],  # Sequence 1 (padded with 0s)\n",
    "    [4, 5, 6, 7, 0],  # Sequence 2\n",
    "    [8, 9, 0, 0, 0]   # Sequence 3\n",
    "]\n",
    "labels = [0, 1, 0]  # Binary labels\n",
    "\n",
    "sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "labels = torch.tensor(labels, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "dataset = SimpleDataset(sequences, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size,embed_dim, hidden_dim, output_dim):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embeddings(x)\n",
    "        rnn_out, hidden = self.rnn(embedded)\n",
    "        final_hidden = hidden[-1]\n",
    "        out = self.fc(final_hidden)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "vocab_size = 10\n",
    "embed_dim = 8\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "\n",
    "model = SimpleRNN(vocab_size, embed_dim, hidden_dim, output_dim)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for seq, label in dataloader:\n",
    "        outputs = model(seq)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if ((epoch+1)%2) == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    test_seq = torch.tensor([[4, 5, 6, 7, 0]])\n",
    "    prediction = model(test_seq)\n",
    "    pred_class = (prediction > 0.5).float()\n",
    "    print(pred_class.item())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6bb37dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/10], Loss: 0.6965\n",
      "Epoch [2/10], Loss: 0.6927\n",
      "Epoch [3/10], Loss: 0.6913\n",
      "Epoch [4/10], Loss: 0.6888\n",
      "Epoch [5/10], Loss: 0.6835\n",
      "Epoch [6/10], Loss: 0.6712\n",
      "Epoch [7/10], Loss: 0.6391\n",
      "Epoch [8/10], Loss: 0.5595\n",
      "Epoch [9/10], Loss: 0.4086\n",
      "Epoch [10/10], Loss: 0.2272\n",
      "Test input: 'it's a stunning movie'\n",
      "Predicted class: Negative\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define the dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_seq_size):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_seq_size = max_seq_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.texts[idx].split(\" \")\n",
    "        sequence = [self.vocab[token] if token in self.vocab else self.vocab[\"<UNK>\"] for token in tokens]\n",
    "\n",
    "        # Pad or truncate to match the max sequence size\n",
    "        if len(sequence) < self.max_seq_size:\n",
    "            sequence += [self.vocab[\"<PAD>\"]] * (self.max_seq_size - len(sequence))\n",
    "        elif len(sequence) > self.max_seq_size:\n",
    "            sequence = sequence[:self.max_seq_size]\n",
    "\n",
    "        return torch.tensor(sequence, dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "\n",
    "\n",
    "def build_vocab(texts):\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    index = 2\n",
    "\n",
    "    for text in texts:\n",
    "        for token in text.split(\" \"):\n",
    "            if token not in vocab:\n",
    "                vocab[token] = index\n",
    "                index += 1\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# Step 2: Generate synthetic dataset\n",
    "np.random.seed(0)\n",
    "\n",
    "positive_texts = [\n",
    "    \"I loved the movie it was fantastic\",\n",
    "    \"Absolutely wonderful experience\",\n",
    "    \"Highly recommend this movie\",\n",
    "    \"The movie was amazing\",\n",
    "    \"I enjoyed the film\",\n",
    "    \"The actors were great\",\n",
    "    \"The plot was engaging\",\n",
    "    \"The movie was well-made\",\n",
    "    \"I would watch it again\",\n",
    "    \"The film was outstanding\"\n",
    "]\n",
    "\n",
    "negative_texts = [\n",
    "    \"The film was terrible and boring\",\n",
    "    \"Worst movie I have ever seen\",\n",
    "    \"The movie was disappointing\",\n",
    "    \"I didn't like the film\",\n",
    "    \"The actors were bad\",\n",
    "    \"The plot was confusing\",\n",
    "    \"The movie was poorly made\",\n",
    "    \"I wouldn't watch it again\",\n",
    "    \"The film was average\",\n",
    "    \"The movie was not good\"\n",
    "]\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    if np.random.rand() > 0.5:\n",
    "        texts.append(np.random.choice(positive_texts))\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        texts.append(np.random.choice(negative_texts))\n",
    "        labels.append(0)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"text\": texts,\n",
    "    \"label\": labels\n",
    "})\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split the DataFrame into training and testing sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_texts = df[\"text\"][:train_size].tolist()\n",
    "train_labels = df[\"label\"][:train_size].tolist()\n",
    "test_texts = df[\"text\"][train_size:].tolist()\n",
    "test_labels = df[\"label\"][train_size:].tolist()\n",
    "\n",
    "# Use the training set for your model\n",
    "texts = train_texts\n",
    "labels = train_labels\n",
    "\n",
    "# Step 3: Define the GRU Model\n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embeddings(x)\n",
    "        gru_out, hidden = self.gru(emb)\n",
    "        final_hidden = hidden[-1]\n",
    "        out = self.fc(final_hidden)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Step 4: Build Vocabulary and Hyperparameters\n",
    "vocab = build_vocab(texts)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "max_len = 10\n",
    "embed_dim = 8\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TextDataset(texts=texts, labels=labels, vocab=vocab, max_seq_size=max_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model and move it to the device\n",
    "model = GRUNet(vocab_size, embed_dim, hidden_dim, output_dim).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for seq, label in dataloader:\n",
    "        # Move inputs and labels to the device\n",
    "        seq, label = seq.to(device), label.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(seq)\n",
    "        loss = criterion(y_pred, label.unsqueeze(1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"gru_model.pth\")\n",
    "\n",
    "# Testing the model\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    test_sentence = \"it's a stunning movie\"\n",
    "    tokens = test_sentence.split(\" \")\n",
    "    seq = [vocab[token] if token in vocab else vocab[\"<UNK>\"] for token in tokens]\n",
    "    seq = seq[:max_len] + [vocab[\"<PAD>\"]] * (max_len - len(seq))\n",
    "    test_seq = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)  # Move to device\n",
    "    y_pred = model(test_seq)\n",
    "    pred_class = (y_pred > 0.5).float()\n",
    "    print(f\"Test input: '{test_sentence}'\")\n",
    "    print(f\"Predicted class: {'Positive' if pred_class.item() == 1 else 'Negative'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c919427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from collections import Counter \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "027ed259",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a655e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b2f38a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[one, of, the, other, reviewers, has, mentione...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[a, wonderful, little, production., &lt;br, /&gt;&lt;br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[i, thought, this, was, a, wonderful, way, to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[basically, there's, a, family, where, a, litt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[petter, mattei's, \"love, in, the, time, of, m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                              tokens  \n",
       "0  [one, of, the, other, reviewers, has, mentione...  \n",
       "1  [a, wonderful, little, production., <br, /><br...  \n",
       "2  [i, thought, this, was, a, wonderful, way, to,...  \n",
       "3  [basically, there's, a, family, where, a, litt...  \n",
       "4  [petter, mattei's, \"love, in, the, time, of, m...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"r'[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "df[\"tokens\"] = df[\"review\"].apply(preprocess_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36440cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[one, of, the, other, reviewers, has, mentione...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[a, wonderful, little, production., &lt;br, /&gt;&lt;br...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[i, thought, this, was, a, wonderful, way, to,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[basically, there's, a, family, where, a, litt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[petter, mattei's, \"love, in, the, time, of, m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                              tokens  label  \n",
       "0  [one, of, the, other, reviewers, has, mentione...      1  \n",
       "1  [a, wonderful, little, production., <br, /><br...      1  \n",
       "2  [i, thought, this, was, a, wonderful, way, to,...      1  \n",
       "3  [basically, there's, a, family, where, a, litt...      0  \n",
       "4  [petter, mattei's, \"love, in, the, time, of, m...      1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocab(tokenized_texts, max_vocab_size=10000):\n",
    "    words_count = Counter(word for tokens in tokenized_texts for word in tokens)\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for word, _ in words_count.most_common(max_vocab_size - 2):\n",
    "        vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "vocab = build_vocab(df[\"tokens\"])\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "label_map = {\"positive\": 1, \"negative\": 0}\n",
    "df[\"label\"] = df[\"sentiment\"].map(label_map)\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01699cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df[\"tokens\"], df[\"label\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c9dbe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.texts.iloc[idx]\n",
    "        sequence = [self.vocab[token] if token in self.vocab else self.vocab[\"<UNK>\"] for token in tokens]\n",
    "\n",
    "        # Pad or truncate to match the max sequence size\n",
    "        if len(sequence) < self.max_len:\n",
    "            sequence += [self.vocab[\"<PAD>\"]] * (self.max_len - len(sequence))\n",
    "        elif len(sequence) > self.max_len:\n",
    "            sequence = sequence[:self.max_len]\n",
    "\n",
    "        return torch.tensor(sequence, dtype=torch.long), torch.tensor(self.labels.iloc[idx], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b834bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 200\n",
    "embed_dim = 128\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9db3a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(train_texts, train_labels, vocab=vocab, max_len=max_seq_len)\n",
    "test_dataset = IMDBDataset(test_texts, test_labels, vocab=vocab, max_len=max_seq_len)\n",
    "\n",
    "\n",
    "train_data = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_data  = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "028723be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, vocab, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(GRU, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        _, hidden = self.gru(x)\n",
    "        final_hidden = hidden[-1] \n",
    "        output = self.fc(final_hidden)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "082c239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device: \", device)\n",
    "model =  GRU(vocab, vocab_size, embed_dim, hidden_dim, output_dim).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eaf39402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [0/5], Loss: 0.6801\n",
      "Epoch : [1/5], Loss: 0.3894\n",
      "Epoch : [2/5], Loss: 0.2573\n",
      "Epoch : [3/5], Loss: 0.1769\n",
      "Epoch : [4/5], Loss: 0.1000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0 \n",
    "    \n",
    "    for text, label in train_data:\n",
    "        text, label = text.to(device), label.to(device)\n",
    "        \n",
    "        y_pred = model(text)\n",
    "        \n",
    "        loss = criterion(y_pred.squeeze(1), label)\n",
    "        \n",
    "        # reset grads\n",
    "        optimizer.zero_grad()\n",
    "        # calculate grads\n",
    "        loss.backward()\n",
    "        # update grads\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch : [{epoch+1}/{epochs}], Loss: {running_loss/len(train_data):.4f}\")\n",
    "    torch.save(model.state_dict(), f\"gru_model_{epoch+1}.pth\")\n",
    "    \n",
    "torch.save(model.state_dict(), \"imdb_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "85a9d68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76064/2864941614.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(\"imdb_model.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Test Accuracy: 0.8605\n"
     ]
    }
   ],
   "source": [
    "model_state_dict = torch.load(\"imdb_model.pth\")\n",
    "\n",
    "model = GRU(vocab=vocab, vocab_size=vocab_size, embed_dim=embed_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "print(device) \n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text, label in test_data:\n",
    "        text, label  = text.to(device), label.to(device)\n",
    "        y_pred = model(text)\n",
    "        \n",
    "        predictions = (y_pred.squeeze(1) > 0.5).float()\n",
    "        total += label.size(0)\n",
    "        \n",
    "        correct += (predictions == label).sum().item()\n",
    "        \n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f17d5fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, text, vocab, max_len, device):\n",
    "    model.eval()\n",
    "    sequence = [vocab.get(token, vocab[\"<UNK>\"]) for token in text.split()]\n",
    "    if len(sequence) < max_len:\n",
    "        sequence += [vocab[\"<PAD>\"]] * (max_len - len(sequence))\n",
    "    else:\n",
    "        sequence = sequence[:max_len]\n",
    "    \n",
    "    input_tensor = torch.tensor(sequence, dtype=torch.long).unsqueeze(0).to(device)  \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_tensor)\n",
    "    \n",
    "    return prediction.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bcaf2dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment score:  0.0018544727936387062\n",
      "Verdict: 'Negative'\n"
     ]
    }
   ],
   "source": [
    "text = \"This movie is bad but I liked the songs anyway but the movie is worst\"\n",
    "sentiment_score = predict_sentiment(model, text, vocab, max_seq_len, device)\n",
    "print(\"sentiment score: \", sentiment_score)\n",
    "label = \"Positive\" if sentiment_score >= 0.5 else \"Negative\"\n",
    "print(f\"Verdict: '{label}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
